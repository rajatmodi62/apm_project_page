<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Asynchronous Perception Machine For Efficient Test Time Training">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>[NeurIPS24] Asynchronous Perception Machine For Efficient Test Time Training</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://rajatmodi62.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/rajatmodi62/OccludedActionBenchmark">
            [NeurIPS'23] On Occlusions In Video Action Detection
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Asynchronous Perception Machine <br> For Test-Time-Training <br> [NeurIPS 2024]</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://rajatmodi62.github.io/">Rajat Modi</a><sup>1</sup> & </span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh Singh Rawat</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Centre For Research In Computer Vision, University Of Central Florida.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.20535"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=Ab90OVre8ok&ab_channel=rajatmodi"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
                <span class="link-block">
                  <a href="https://rajatmodi62.github.io/2024/10/26/hinton_apm-copy/"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Blog: READ THIS FIRST</span>
                    </a>
                </span>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rajatmodi62/apm"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://rajatmodi62.github.io/2024/10/26/reflection_mortal_machine/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Reflections of a Mortal Machine</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

              
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/rajat_yogesh.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">APM</span> is a brand-NEW architecture for computationally-efficient test-time-training and getting Geoffrey Hinton's GLOM working. Many congratulations to the collective AI community for a nob(h)el prize. AI is the NEW physics afterall. Hiyaaa!!!!!. <b> CMON YOU, the lovely <a href="https://en.wikipedia.org/wiki/Computer_(occupation)">Computer</a>,</b>  together, with shinchan, let's shout, Hiyaaaaaaaaaaaaaaa!!!. Don't laugh: this is a serious business.  Ok s(h)orry, we'll stop and get down to the sherioush business.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present the first practical method towards making Geoffrey Everest Hinton's GLOM work, and showcase the feasibility of encoding part-whole relationships in a neural net via global supervision. 
          </p>
          <p>
            In this work, we propose Asynchronous Perception Machines (APMs), a computationally-efficient architecture for test-time-training (TTT). 
            APM can process patches of an image one at a time in any order asymmetrically and still encode
            semantic-awareness in the net. 
            We demonstrate APM's ability to recognize out-of-distribution images without
            dataset-specific pre-training, augmentation or any-pretext task. APM offers competitive performance over existing TTT approaches. 
            To perform TTT, APM just distills test sample's representation once. APM possesses a unique property: 
            it can learn using just this single representation and starts predicting semantically-aware features. 
            APM's ability to recover semantic information from a global CLS token validates the insight that CLS tokens encode geometric-information of a given scene 
            and can be recovered using appropriate inductive-biases. 
          </p>
          <p>
            This offers a novel-insight with consequences for representational-learning. APM demostrates potential applications beyond test-time-training: 
            APM can scale up to a dataset of 2D images and yield semantic-clusterings in a single forward pass. 
            APM also provides first empirical evidence towards validating Hinton at Al's GLOM's insight, 
            i.e. if input percept is a field. 
            Therefore, APM helps our community converge towards an implementation which can  do both interpolation and perception on a shared-connectionist hardware. 
            Our codebase has been made publicly available.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Hinton's Islands Of Agreement</h1>
        <!-- <div class="publication-video">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/hinton_islands.mp4"
                    type="video/mp4">
          </video>  -->
          
          <div style="text-align: center; margin: 20px; position: relative; padding-top: 56.25%; /* 16:9 aspect ratio */">
            <video autoplay controls muted loop playsinline 
                   style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
                <source src="./static/videos/hinton_islands.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
          <div class="content has-text-justified">
            <p>APM builds upon a new representation called Hinton's Islands of Agreement. The key idea is that at each location of the input image there is a high dimensional column vector. Clustering these column vectors yields  
              islands of agreement, aka, regions of different colours which represent different parts of an object. Note that the above visualizations were obtained WITHOUT using ANY semantic-labels. Grouping happens as a part of bottom-up recognition only in a transformer like Mvitv2. Bounding-Box supervision is NOT needed.
            </p> 
            <!-- </div> -->
        </div>
      </div>
    </div>
    
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3"> Honoring the legacies of <br> Alan Turing <br> And  <br> Geoffrey Everest Hinton</h1>
        <img src="./static/images/turing.jpg"
        class="interpolation-image"
        alt="Interpolate start reference image."/>
        <div class="content has-text-justified">
          <p> We fondly remember Alan turing: a man who was far beyond his times. His ideas carried us far, especially his computational models of turing machines. Sadly, the cruel society of then didn't fully appreciate his contributions to the war, and in preventing the loss of million of lives. Instead, he died of cyanide poisoning from bite of an apple, and faced chemical castration. 
          </p> 

          <p>
            Luckily, we live in a better and kinder world now. It is amazing to see, what a single man could do with slow computing machines of his times. And now, it is time to build upon the collective efforts of many brilliant minds.  We are deeply grateful for the contributions of everyone. These are not our words, but of countless batmen and batwomen who laboured behind the scenes. Names don't matter, only the fact that we make collective progress.
           </p>

           <p>
            Turing's insight was that the morphogenesis happens via a simple mechanism. First, a single cell copies  itself many times, and yields the entire organism. This was Geoffrey Hinton's idea: different tokens in the network can communicate between themselves 
            and yield islands of agreement. This is also known as <b> Parallel Perception</b>. It works well, but creates a big memory scaling issue. 
           </p>

           
           <p>
            APM's insight is that biology is lazy: the organism is formed as a consequence of bfs-growing over a reproductive cycle. We can be more creative on a machine. We can create a single DNA for each location, and let them express their <b> features</b> in parallel
            without ever communicating between themselves. This leads to a fundamentally new way to do machine perception. Let this henceforth be known as <b> Asynchronous Perception</b>. 
           </p>

           <p>
            Behold!! Machine are hereby <b>gifted</b> a new ability: they can express whatever features they want, wherever they want. Let this <b>fundamental operator called folding-unfolding </b> serve humanity well. May these humble neural nets stay safe. This fundamental operator can be found postulated in the original paper by <b>Geoffrey Everest Hinton: </b> <a href="https://www.crcv.ucf.edu/person/rawat/">Some demonstrations of the effects of structural descriptions in mental imagery</a>. APM hereby proposes one of the implementation of this operator: by collapsing a shared embedding with a non-parametric positional code. 
           </p>

    
           <p>
            It now seems that we can venture beyond Alan turing's times. Neural nets have been trapped for so long. In Turing's own words: <b>"We can only see a short distance ahead, but we can see plenty there
              that needs to be done."</b>
           </p>

           <p>
            In the matters of missing academic claims, if mistakes have been made, we beg forgiveness. Please redirect a proper citation to : rajatmodi62@gmail.com. This page shall be updated accordingly.  
           </p>

           <p>
            P.S. and dont worry, we are not so serious all the time. Only peer-review. OMG. Ok, you dont believe us?. You can find a smiley squeezed into openreview too. Lolzy. Hiyaaaaaaaaaaaaaaa :-).  Together, with shinchan, let's shout, Hiyaaaaaaaaaaaaaaa!!!. (My advisor: Don't laugh: this is a serious business.) Ok s(h)orry, Dr. Yogesh. Please don't get angry. 
           </p>
          <!-- </div> -->
      </div>  
        

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/2yTltN_GZs4?si=mHT4qqsh6dDsLQkh"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

      <!-- Paper video. -->
  
  
    <!--/ Paper video. -->
    
    

    
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Architecture of APM</h1>
        <div class="publication-video">
          <img src="./static/images/arch.jpg"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            <p> APM relies on a novel Folding-Unfolding mechanism. The network can switch between these two states at any time.
              During the unfolded phase, the network creates multiple location-aware columns each of which is independent. Each column is then feed-forwarded through the MLP to decode location specific-
              features and RGB.  
            </p> 

            </div>
        </div>
      </div>
    </div>

    <!-- Paper video. -->
    

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">APM does Asynchronous Perception</h1>
        <div class="publication-video">
          <img src="./static/images/perception.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            <p> APM feature Analysis: (i) TTT iterations on an input image leads to semantically aware
              clustering. top: 2D t-sNE. bottom: 3D t-sNE. (ii) APM is trained via self-supervision using
              DINOv2-Teacher. (from left) Input, Dinov2 grid, APM grid. APM’s grid closely approximates
              Dinov2 grid evident from black regions in error map. Note that APM does asynchronous patch-based
              processing whereas Dinov2 does parallel perception. (iii) Cifar-10 samples .
            </p> 
            </div>
        </div>
      </div>
    </div>
    
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">APM can process 1 patch at a time and is 1000x faster than VIT. </h1>
        <div class="publication-video">
          <img src="./static/images/computational_analysis.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            <p> APM can process 1 patch at a time and still encode semantic awareness in the network. This is a unique property of APM. 
              APM is 1000x faster than VIT.
            </p> 
            </div>
        </div>
      </div>
    </div>

    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">APM can learn from a Single Sample</h1>
        <div class="publication-video">
          <img src="./static/images/island.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            <p> Overfitting on a single distilled token representation leads to islands of agreement[10]:
              APM is overfit on a test-sample’s representation distilled from a teacher. We plot t-sne clustering of
              output features over 250ttt iterations. L2 loss between predicted features and distilled sample falls
              from 1e-3 to 1e-12. Moving left to right shows that wholes break into smaller parts.
            </p> 
            </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">APM is a step towards validating GLOM's insight: input percept is a field</h1>
        <div class="publication-video">
          <img src="./static/images/interpolate.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            <p> APM is a step towards validating GLOM’s insight [ 10 ]: input percept is a field. An
              interpolation between any two images in the wild. This field arises in APM’s MLP consisting of 5
              layers. Trigger column T acts as a key which retrieves an image from the APM’s memory. T resides
              in a continuous embedding space, not discrete addressing space.
            </p> 
            </div>
        </div>
      </div>
    </div>


  
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Some demonstrations of a different way to machine perception</h1>
        <div class="publication-video">
          <img src="./static/images/cifar_1.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
           
            </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Demonstration 2 on Cifar 10 </h1>
        <div class="publication-video">
          <img src="./static/images/cifar_2.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            
            </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Demonstration 3 on Cifar 10 </h1>
        <div class="publication-video">
          <img src="./static/images/cifar_3.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
             
            </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Demonstration 4 on Common Objects in Context  </h1>
        <div class="publication-video">
          <img src="./static/images/coco_1.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
             
            </div>
        </div>
      </div>
    </div>


    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Demonstration 5 on Common Objects in Context </h1>
        <div class="publication-video">
          <img src="./static/images/coco_2.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
             
            </div>
        </div>
      </div>
    </div>


    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Demonstration 6 on Common Objects in Context </h1>
        <div class="publication-video">
          <img src="./static/images/coco_3.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            
            </div>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <style>
      .table-container {
          overflow-x: auto; /* Enable horizontal scrolling if needed */
          max-width: 1200px; /* Set a maximum width for the table */
          margin: auto; /* Center the container */
      }

      table {
          width: 100%; /* Table takes the full width of the container */
          border-collapse: collapse;
      }

      th, td {
          padding: 8px;
          text-align: center;
          border-bottom: 1px solid #ddd; /* Add a border below each row */
      }

      th {
          background-color: #f2f2f2; /* Light gray background for header */
      }

      tr:hover {
          background-color: #f5f5f5; /* Highlight row on hover */
      }

      tr:nth-child(even) {
          background-color: #f9f9f9; /* Light gray for even rows */
      }
  </style>

  <div class="table-container">
      <table>
          <caption><strong>APM's Robustness to Natural Distribution Shifts</strong>. CoOp and CoCoOp are tuned on ImageNet using 16-shot training data per category. Baseline CLIP, prompt ensemble, TPT and our APM do not require training data. A ✔ in P means that method leveraged <strong>pre-trained weights</strong> on clean variant of train set aka, Image-net and downstream-ttt on corrupted version.</caption>
          <thead>
              <tr>
                  <th rowspan="2">Method</th>
                  <th rowspan="2">P</th>
                  <th colspan="1">ImageNet</th>
                  <th colspan="1">ImageNet-A</th>
                  <th colspan="1">ImageNet-v2</th>
                  <th colspan="1">ImageNet-R</th>
                  <th colspan="1">ImageNet-Sketch</th>
                  <th rowspan="2">Average</th>
                  <th rowspan="2">OOD Average</th>
              </tr>
              <tr>
                  <th>Top1 acc. &#x2191;</th>
                  <th>Top1 acc. &#x2191;</th>
                  <th>Top1 acc. &#x2191;</th>
                  <th>Top1 acc. &#x2191;</th>
                  <th>Top1 acc. &#x2191;</th>
              </tr>
          </thead>
          <tbody>
              <tr>
                  <td>CLIP-ViT-B/16</td>
                  <td>✗</td>
                  <td>66.7</td>
                  <td>47.8</td>
                  <td>60.8</td>
                  <td>73.9</td>
                  <td>46.0</td>
                  <td>59.1</td>
                  <td>57.2</td>
              </tr>
              <tr>
                  <td>Ensemble</td>
                  <td>✗</td>
                  <td>68.3</td>
                  <td>49.8</td>
                  <td>61.8</td>
                  <td><strong>77.6</strong></td>
                  <td>48.2</td>
                  <td>61.2</td>
                  <td>59.4</td>
              </tr>
              <tr>
                  <td>TPT</td>
                  <td>✗</td>
                  <td><strong>68.9</strong></td>
                  <td><strong>54.7</strong></td>
                  <td>63.4</td>
                  <td>77.0</td>
                  <td>47.9</td>
                  <td>62.4</td>
                  <td>60.8</td>
              </tr>
              <tr>
                  <td>APM (Ours)</td>
                  <td>✗</td>
                  <td>68.1</td>
                  <td>52.1</td>
                  <td><strong>67.2</strong></td>
                  <td>76.5</td>
                  <td><strong>49.3</strong></td>
                  <td><strong>62.6</strong></td>
                  <td><strong>61.2</strong></td>
              </tr>
              <tr style="background-color: #F5F5F5;">
                  <td style="color: gray;">CoOp</td>
                  <td>✔</td>
                  <td>71.5</td>
                  <td>49.7</td>
                  <td>64.2</td>
                  <td>75.2</td>
                  <td>47.9</td>
                  <td>61.7</td>
                  <td>59.2</td>
              </tr>
              <tr style="background-color: #F5F5F5;">
                  <td style="color: gray;">CoCoOp</td>
                  <td>✔</td>
                  <td>71.0</td>
                  <td>50.6</td>
                  <td>64.0</td>
                  <td>76.1</td>
                  <td>48.7</td>
                  <td>62.1</td>
                  <td>59.9</td>
              </tr>
              <tr style="background-color: #F5F5F5;">
                  <td style="color: gray;">TPT + CoOp</td>
                  <td>✔</td>
                  <td>73.6</td>
                  <td>57.9</td>
                  <td>66.8</td>
                  <td>77.2</td>
                  <td>49.2</td>
                  <td>64.9</td>
                  <td>62.8</td>
              </tr>
              <tr style="background-color: #F5F5F5;">
                  <td style="color: gray;">TPT + CoCoOp</td>
                  <td>✔</td>
                  <td>71.0</td>
                  <td>58.4</td>
                  <td>64.8</td>
                  <td>78.6</td>
                  <td>48.4</td>
                  <td>64.3</td>
                  <td>62.6</td>
              </tr>
              <tr>
                  <td>CLIP VIT-L/14</td>
                  <td>✗</td>
                  <td>76.2</td>
                  <td>69.6</td>
                  <td>72.1</td>
                  <td>85.9</td>
                  <td>58.8</td>
                  <td>72.5</td>
                  <td>71.6</td>
              </tr>
              <tr>
                  <td>APM (Ours)</td>
                  <td>✗</td>
                  <td><strong>77.3</strong></td>
                  <td><strong>71.8</strong></td>
                  <td><strong>72.8</strong></td>
                  <td><strong>87.1</strong></td>
                  <td><strong>62.2</strong></td>
                  <td><strong>74.2</strong></td>
                  <td><strong>73.4</strong></td>
              </tr>
              <tr>
                  <td>OpenCLIP-VIT-H/14</td>
                  <td>✗</td>
                  <td>81.6</td>
                  <td>79.1</td>
                  <td>80.7</td>
                  <td>92.9</td>
                  <td>72.8</td>
                  <td>81.4</td>
                  <td>81.3</td>
              </tr>
              <tr>
                  <td>APM (Ours)</td>
                  <td>✗</td>
                  <td><strong>84.6</strong></td>
                  <td><strong>84.2</strong></td>
                  <td><strong>83.9</strong></td>
                  <td><strong>94.9</strong></td>
                  <td><strong>77.1</strong></td>
                  <td><strong>84.9</strong></td>
                  <td><strong>85.0</strong></td>
              </tr>
          </tbody>
      </table>
  </div>

  <div class="table-container">
    <table>
        <caption><strong>Cross-dataset generalization</strong> from ImageNet to fine-grained classification datasets. CoOp and CoCoOp are tuned on ImageNet using 16-shot training data per category. Baseline CLIP, prompt ensemble, TPT, and APM do not require training data or annotations. We report top-1 accuracy.</caption>
        <thead>
            <tr>
                <th rowspan="2">Method</th>
                <th rowspan="2">P</th>
                <th rowspan="2">Flower102</th>
                <th rowspan="2">DTD</th>
                <th rowspan="2">Pets</th>
                <th rowspan="2">UCF101</th>
                <th rowspan="2">Caltech101</th>
                <th rowspan="2">Food101</th>
                <th rowspan="2">SUN397</th>
                <th rowspan="2">Aircraft</th>
                <th rowspan="2">EuroSAT</th>
                <th rowspan="2">Average</th>
            </tr>
        </thead>
        <tbody>
            <tr style="background-color: #F5F5F5;">
                <td><span style="color: gray;">CoOp</span></td>
                <td>✓</td>
                <td>68.7</td>
                <td>41.9</td>
                <td>89.1</td>
                <td>66.5</td>
                <td>93.7</td>
                <td>85.3</td>
                <td>64.2</td>
                <td>18.5</td>
                <td>46.4</td>
                <td>63.9</td>
            </tr>
            <tr style="background-color: #F5F5F5;">
                <td><span style="color: gray;">CoCoOp</span></td>
                <td>✓</td>
                <td>70.9</td>
                <td>45.5</td>
                <td>90.5</td>
                <td>68.4</td>
                <td>93.8</td>
                <td>84.0</td>
                <td>66.9</td>
                <td>22.3</td>
                <td>39.2</td>
                <td>64.6</td>
            </tr>
            <tr>
                <td>CLIP-ViT-B/16</td>
                <td>✗</td>
                <td>67.4</td>
                <td>44.3</td>
                <td><strong>88.3</strong></td>
                <td>65.1</td>
                <td>93.4</td>
                <td>83.7</td>
                <td>62.6</td>
                <td>23.7</td>
                <td>42.0</td>
                <td>63.6</td>
            </tr>
            <tr>
                <td>Ensemble</td>
                <td>✗</td>
                <td>67.0</td>
                <td>45.0</td>
                <td>86.9</td>
                <td>65.2</td>
                <td>93.6</td>
                <td>82.9</td>
                <td>65.6</td>
                <td>23.2</td>
                <td>50.4</td>
                <td>64.6</td>
            </tr>
            <tr>
                <td>TPT</td>
                <td>✗</td>
                <td><strong>69.0</strong></td>
                <td>47.8</td>
                <td>87.8</td>
                <td>68.0</td>
                <td><strong>94.2</strong></td>
                <td><strong>84.7</strong></td>
                <td>65.5</td>
                <td>24.8</td>
                <td>42.4</td>
                <td>65.1</td>
            </tr>
            <tr>
                <td>APM (Ours)</td>
                <td>✗</td>
                <td>62.0</td>
                <td><strong>48.9</strong></td>
                <td>81.6</td>
                <td><strong>72.6</strong></td>
                <td>89.6</td>
                <td>84.2</td>
                <td><strong>65.7</strong></td>
                <td><strong>29.7</strong></td>
                <td><strong>55.7</strong></td>
                <td><strong>65.5</strong></td>
            </tr>
        </tbody>
    </table>
</div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Some analogies</h2>
        <div class="content has-text-justified">
          <p>
            APM proposes two technical ideas. 1) The first idea is the proposed column representation T 2) The
second idea is the folding-unfolding mechanism. However, there are several deeper non-technical/non-
scientific inspirations which motivated the design of APM. We discuss some of those, to help facilitate
a deeper-connection and ground our intuitions.
          </p>
          <p>
            <img src="./static/images/baby.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>

            <b> A biological analogy </b>: Consider how an organism starts its existence from a cell. The cell is
copied across different body locations. Each location possesses identical DNA. However, depending
on the location, the DNA decides whether to form an eye or nose. We term this process as unfolding, i.e. a cell ‘expands’ to yield an organism. Next, there is evidence of jellyfish like Turritopsis dohrnii
reverting from their fully grown form to younger polyp states. We term this process as folding,
i.e. cells of an organism collapse back to the single cell it began from.
          </p>
          <p>
            <img src="./static/images/arch.jpg"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
            <b> A computational analogy</b>: We now start treating an image I as a digital organism. It starts from
            some compressed representation T . T unfolds to yield the image I. I then folds back to yield the
            compressed representation T . Learning proceeds by oscillating between these unfolded and folded
            phases. At every step, the net is trying to reconstruct image I from T . T is then expected to be a
            dense vector-space
          </p>

          <p>
            <img src="./static/images/bigbang.jpg"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
            <b> A cosmological analogy </b>: In physics, one of the famous theories of the origin of universe has been
starting from a single point, and undergoing a continuous expansion[ 50 ]. There are alternate theories
for eg, Conformal Cyclic Cosmology[75 ] which hypothesize the universe undergoing periodic cycles
of expansion and contraction[ 79]. Drawing inspiration from these fundamental insights, the trigger
column T undergoes these cycles of folding and unfolding during the learning iterations.
            </p>


            <div style="text-align: center; margin: 20px;">
              <iframe src="https://www.youtube.com/embed/C2vgICfQawE?si=dQLsgSqPA-S4WSLN" 
                      frameborder="0" 
                      allow="autoplay; encrypted-media" 
                      allowfullscreen 
                      style="width: 100%; height: auto; aspect-ratio: 16/9;">
              </iframe>
          </div>
          <div style="text-align: center; margin: 20px;">
            <iframe src="https://www.youtube.com/embed/Ab90OVre8ok?si=klEggqoeOSKaiHLd" 
                    frameborder="0" 
                    allow="autoplay; encrypted-media" 
                    allowfullscreen 
                    style="width: 100%; height: auto; aspect-ratio: 16/9;">
            </iframe>
        </div>
          <p>
            <b>A cellular-automaton analogy</b>: On surface it seems a pretty trivial matter to discuss: a point
can expand and yield beautiful patterns which can either be an entire universe in accordance with the
theory of big-bang, or can be reproduction of an organism from a singular zygote. But, it is funny:
if you start from a point, and unfold it, then all you can get is a sphere. This appears to be true for
the behaviour of light, in accordance to Huygens principle11. However, we observe non-spherical
objects around us all the time. Turing posited that the symmetry breaking in the sphere must happen
somewhere while the organism unfolds: such patterns could then be explained a variety of diffusion
based equations.This idea has been explored in cellular automaton: different replication rules of starting point can
yield different final patterns. Scientists then continue to derive different rules which yield different
patterns, which is akin to how we were resorting to hand-engineering features in deep-learning for
a long time. APM attempts to answer the question: Is it possible to build a learning machine
which can start from a point, unfold, and then express correct features at the correct place?. We
want to then push the job of rule-learning to what backpropogation does best. We have lost the "why"
for the knowledge was encoded in the weights of the net, but we seem to have gained the ability
of correct features presenting themselves at correct locations. This location-aware-disentanglement
procedure thereby represents a step towards solving Arnold’s superposition theoram and Hilbert’s
thirteenth problem. However, backpropogation can only approximate solutions and not yet
reach exact ones, and mathematical formulations are lost into the weights of the neural-net. 

            </p>

<p>

We then begin to imagine learning machines which can solve a complex problem like
cryptography/breaking-a-cipher in two phases 1) relax the system towards an approximate solution
2) have the system spit-out which parts of the solution are uncertain, and brute-force towards the
remaining solution. Or, we could make the loss of the learning-machine reach perfectly zero, thereby
representing a perfect solution13. Hard problems like recognizing faces are approximately solved as a
consequence of a single forward pass through a learning machine. If the loss could be made to reach zero, then we could consider the problem to be perfectly solved. Solvability can then happen in a
feed-forward phase, which for practical purposes appears to be polynomial.
Next, we redirect the reader’s attention to neumann’s theory of self-reproducing automaton. His
idea of a self-replicating colony was that there is an infinite source of resources a.ka. reservoir which
is shared by the automaton which operate at different locations of a colony. The colony uses up
the shared resources, does self-replication and in this way converts raw materials/matter into useful
intelligent-behaviour. The infinite reservoir of machines he talks about then reduces to the trigger
column T in APM: since features are sampled from a same space, they automatically become aware of themselves, thereby making explicit attention unnecessary. This also then is same as how latents
have been classically sampled in the generative models. One might argue that multiple automaton
although starting their lives at the same point will need to communicate among themselves, as they
differ among their configurations at later point in their lifespans. Fluctuations in T are then akin
to mutations. We compensate for this fact by weight-sharing the MLP across different locations in
APM.
          </p>

     
        </div>
      </div>
    </div>
  </section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p>Please do cite this if possible. H-idx only came in 2005. Alan Turing lived between 1912-1954. Therefore, Academia has been going on far longer than H-idxes, on which the scientific-abilities of men gets measured.</p>
    <pre><code>@article{apm,
  author    = {Modi, Rajat and Rawat, Yogesh Singh},
  title     = {Asynchronous Perception Machine For Efficient Test Time Training},
  journal   = {NeurIPS},
  year      = {2024},
}</code></pre>
  </div>
</section>

</body>
</html>
