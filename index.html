<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Asynchronous Perception Machine For Efficient Test Time Training">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>[NeurIPS24] Asynchronous Perception Machine For Efficient Test Time Training</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://rajatmodi62.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/rajatmodi62/OccludedActionBenchmark">
            [NeurIPS'23] On Occlusions In Video Action Detection
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Asynchronous Perception Machine For Efficient Test Time Training</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://rajatmodi62.github.io/">Rajat Modi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh Singh Rawat</a><sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Centre For Research In Computer Vision, University Of Central Florida.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/14KDsBcQx82_4Nhmq9td12A6USrMKcWWk/view?usp=drive_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=Ab90OVre8ok&ab_channel=rajatmodi"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rajatmodi62/apm"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Blog</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

              
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/cover.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">APM</span> is a NEW architecture for computationally-efficient test-time-training and getting Dr. Geoffrey Hinton sir's GLOM working. 
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present the first practical method towards making Dr. Geoff Hinton sir's GLOM work, and showcase the feasibility of encoding part-whole relationships in a neural net via global supervision. 
          </p>
          <p>
            In this work, we propose Asynchronous Perception Machine (APM), a computationally-efficient architecture for test-time-training (TTT). 
            APM can process patches of an image one at a time in any order asymmetrically and still encode
            semantic-awareness in the net. 
            We demonstrate APM's ability to recognize out-of-distribution images without
            dataset-specific pre-training, augmentation or any-pretext task. APM offers competitive performance over existing TTT approaches. 
            To perform TTT, APM just distills test sample's representation once. APM possesses a unique property: 
            it can learn using just this single representation and starts predicting semantically-aware features. 
            APM's ability to recover semantic information from a global CLS token validates the insight that CLS tokens encode geometric-information of a given scene 
            and can be recovered using appropriate inductive-biases. 
          </p>
          <p>
            This offers a novel-insight with consequences for representational-learning. APM demostrates potential applications beyond test-time-training: 
            APM can scale up to a dataset of 2D images and yield semantic-clusterings in a single forward pass. 
            APM also provides first empirical evidence towards validating Hinton at Al's GLOM's insight, 
            i.e. if input percept is a field. 
            Therefore, APM helps our community converge towards an implementation which can  do both interpolation and perception on a shared-connectionist hardware. 
            Our codebase has been made publicly available.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  

    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://youtube.com/clip/UgkxYQ1xAmDhTkESsNCDCbF57LmIo2JToTPk?si=9aVbEflVHJdDsehd"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/Ab90OVre8ok?si=ZKxizi9m5fy2sbSq"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/_RhT4phGo58?si=tQwnLANkbWeDNNV3"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

    
    <!--/ Paper video. -->
    
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Hinton's Islands Of Agreement</h1>
        <div class="publication-video">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/hinton_islands.mp4"
                    type="video/mp4">
          </video>   
          <div class="content has-text-justified">
            <p>APM builds upon a new representation called Hinton's Islands of Agreement. The key idea is that at each location of the input image there is a high dimensional column vector. Clustering these column vectors yields  
              islands of agreement, aka, regions of different colours which represent different parts of an object. Note that the above visualizations were obtained WITHOUT using ANY semantic-labels. Grouping happens as a part of bottom-up recognition only in a transformer like Mvitv2. Bounding-Box supervision is NOT needed.
            </p> 
            </div>
        </div>
      </div>
    </div>

    
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Architecture of APM</h1>
        <div class="publication-video">
          <img src="./static/images/arch.jpg"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            <p> APM relies on a novel architecture which uses a mechanism called Folding-Unfolding. In (ii) the network exists in a "folded-state". This is the state of APM before the start of 
              any forward pass. In (iii) the network is in an "unfolded-state". This is the state of APM at the end of every forward pass. The network can switch between these two states at any time.
              During the unfolded phase, the network creates multiple location-aware columns each of which is independent. Each column is then feed-forwarded through the MLP to decode location specific-
              features and RGB. Thus APM, can work for ANY 2D image, and does not require any complex model like ray-tracing used in Neural Fields. 
            </p> 

            </div>
        </div>
      </div>
    </div>

    <!-- Paper video. -->
    

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">APM does Asynchronous Perception</h1>
        <div class="publication-video">
          <img src="./static/images/perception.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            <p> APM feature Analysis: (i) TTT iterations on an input image leads to semantically aware
              clustering. top: 2D t-sNE. bottom: 3D t-sNE. (ii) APM is trained via self-supervision using
              DINOv2-Teacher. (from left) Input, Dinov2 grid, APM grid. APM’s grid closely approximates
              Dinov2 grid evident from black regions in error map. Note that APM does asynchronous patch-based
              processing whereas Dinov2 does parallel perception. (iii) Cifar-10 samples feed-forwarded through
              SSL-trained APM yields features of significant semantic quality.
            </p> 
            </div>
        </div>
      </div>
    </div>

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">APM can process 1 patch at a time and is 1000x faster than VIT. </h1>
        <div class="publication-video">
          <img src="./static/images/computational_analysis.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            <p> APM can process 1 patch at a time and still encode semantic awareness in the network. This is a unique property of APM. 
              APM is 1000x faster than VIT.
            </p> 
            </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">APM can learn from a Single Sample</h1>
        <div class="publication-video">
          <img src="./static/images/island.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            <p> Overfitting on a single distilled token representation leads to islands of agreement[10]:
              APM is overfit on a test-sample’s representation distilled from a teacher. We plot t-sne clustering of
              output features over 250ttt iterations. L2 loss between predicted features and distilled sample falls
              from 1e-3 to 1e-12. Moving left to right shows that wholes break into smaller parts.
            </p> 
            </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Test Time Training Results on Imagenet Splits</h1>
        <div class="publication-video">
          <img src="./static/images/ttt_imagenet.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Test Time Training results on cross-generalization experiments</h1>
        <div class="publication-video">
          <img src="./static/images/ttt_cross_data_generalization.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{apm,
  author    = {Modi, Rajat and Rawat, Yogesh Singh},
  title     = {Asynchronous Perception Machine For Efficient Test Time Training},
  journal   = {NeurIPS},
  year      = {2024},
}</code></pre>
  </div>
</section>



</body>
</html>
